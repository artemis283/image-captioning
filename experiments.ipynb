{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModel, CLIPTextModel, AutoTokenizer\n",
    "print(PIL.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 24811, Test size: 6203\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "ds = load_dataset(\"nlphuji/flickr30k\")\n",
    "test_dataset = ds['test']\n",
    "\n",
    "def apply_transform(example):\n",
    "    example['image'] = transform(example['image'])\n",
    "    return example\n",
    "\n",
    "\n",
    "# splitting the dataset as it only has one split\n",
    "split_dataset = test_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=417x500 at 0x287197200>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=333x500 at 0x287197950>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x323 at 0x287195DF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=476x500 at 0x2871958E0>], 'caption': [['Three people are standing on the dock of a boat looking at something off-camera.', 'Three men stand atop a boat from Seattle Washington.', 'Three people stand on a boat that is docked.', 'A blue and white sailboat sits by a dock.', 'A few people park their boat at a dock.'], ['A man with a gray beard and a little boy are sitting on the floor looking over some papers in a room with a bunk bed.', 'A man in a blue shirt playing with a young boy in a red shirt on the ground in a bedroom.', \"A dad and his son are playing with some Legos in the child's bedroom.\", 'A man and bot playing on the floor with bunk beds in the background.', \"Father and son are in the child's room, putting together a toy.\"], ['A man in a black shirt and gray hat gives the thumbs up to a crowd with one hand, while holding a microphone in the other.', 'A man with a scruffy beard and a hat appears to be giving a speech to a crowd holding a microphone.', 'A man wearing a blue hat gives two thumbs up at a crowd of people.', 'A man is giving two thumbs up to a crowd of people.', 'A man giving two thumbs up to a crowd.'], ['A boy in the middle of a pitch at a baseball game.', 'A young pitcher is throwing the baseball.', 'A pitcher on a mound throws a baseball.', 'A baseball player throws the ball.', 'A teenage boy pitching a baseball.']], 'sentids': [['17530', '17531', '17532', '17533', '17534'], ['80270', '80271', '80272', '80273', '80274'], ['134130', '134131', '134132', '134133', '134134'], ['69745', '69746', '69747', '69748', '69749']], 'split': ['train', 'train', 'train', 'train'], 'img_id': ['3506', '16054', '26826', '13949'], 'filename': ['2125216241.jpg', '372160542.jpg', '5738179350.jpg', '3471463779.jpg']}\n",
      "torch.Size([3, 323, 500])\n",
      "['Three people are standing on the dock of a boat looking at something off-camera.', 'Three men stand atop a boat from Seattle Washington.', 'Three people stand on a boat that is docked.', 'A blue and white sailboat sits by a dock.', 'A few people park their boat at a dock.']\n",
      "<class 'list'>\n",
      "tensor([[[0.2431, 0.5373, 0.5059,  ..., 0.5020, 0.5294, 0.2510],\n",
      "         [0.5647, 1.0000, 1.0000,  ..., 0.9843, 1.0000, 0.5725],\n",
      "         [0.5137, 0.9922, 0.9216,  ..., 0.9490, 0.9843, 0.5059],\n",
      "         ...,\n",
      "         [0.3490, 0.6196, 0.3137,  ..., 0.8235, 0.9490, 0.4784],\n",
      "         [0.3686, 0.7608, 0.8667,  ..., 0.8706, 1.0000, 0.5333],\n",
      "         [0.1333, 0.3804, 0.3843,  ..., 0.4627, 0.5216, 0.2431]],\n",
      "\n",
      "        [[0.2510, 0.5373, 0.5059,  ..., 0.5020, 0.5294, 0.2510],\n",
      "         [0.5608, 0.9922, 0.9922,  ..., 0.9843, 1.0000, 0.5725],\n",
      "         [0.5098, 0.9765, 0.8980,  ..., 0.9529, 0.9882, 0.5059],\n",
      "         ...,\n",
      "         [0.2784, 0.4314, 0.1961,  ..., 0.7569, 0.9098, 0.4667],\n",
      "         [0.2824, 0.4941, 0.5804,  ..., 0.7608, 0.9294, 0.4980],\n",
      "         [0.1333, 0.2784, 0.2627,  ..., 0.4039, 0.4824, 0.2235]],\n",
      "\n",
      "        [[0.2392, 0.5451, 0.4667,  ..., 0.4941, 0.5294, 0.2510],\n",
      "         [0.5804, 1.0000, 0.9804,  ..., 0.9843, 1.0000, 0.5725],\n",
      "         [0.4902, 0.9647, 0.8431,  ..., 0.9294, 0.9686, 0.5059],\n",
      "         ...,\n",
      "         [0.2000, 0.2510, 0.0863,  ..., 0.6549, 0.8039, 0.4392],\n",
      "         [0.1882, 0.3176, 0.3804,  ..., 0.6039, 0.7608, 0.4784],\n",
      "         [0.1020, 0.1882, 0.1804,  ..., 0.3765, 0.4431, 0.2078]]])\n"
     ]
    }
   ],
   "source": [
    "first_4_images = train_dataset[:4]\n",
    "\n",
    "print(first_4_images)\n",
    "\n",
    "images = [transform(image) for image in first_4_images['image']]\n",
    "captions = first_4_images['caption']\n",
    "\n",
    "\n",
    "print(images[2].shape)\n",
    "print(captions[0])\n",
    "print(type(captions[0]))\n",
    "\n",
    "print(images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n",
      "torch.Size([1, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = images[0]\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(last_hidden_state.shape)\n",
    "\n",
    "\n",
    "#Â making images the same embedding dim as caption\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "projection = Projection(768, 512)\n",
    "image_embedding = projection(last_hidden_state)\n",
    "print(image_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three people are standing on the dock of a boat looking at something off-camera.\n",
      "CLIPTextModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPSdpaAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 19, 512])\n"
     ]
    }
   ],
   "source": [
    "# take only the embedding layer not the hidden\n",
    "caption = captions[0][0]\n",
    "print(caption)\n",
    "\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "print(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "caption_inputs = tokenizer(caption, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "input_ids = caption_inputs['input_ids']\n",
    "position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "token_embeddings = model.text_model.embeddings.token_embedding(input_ids)\n",
    "position_embeddings = model.text_model.embeddings.position_embedding(position_ids)\n",
    "\n",
    "input_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "print(input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only\n",
    "# Masked multi-head attention\n",
    "import math\n",
    "\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size, max_seq_len, num_heads=1, bias=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "        \"\"\"arguments: \n",
    "        embedding_dim = size of embedding dimension\n",
    "        num_heads = number of attention heads\n",
    "        max_seq_len = maximum sequence length\n",
    "        bias = whether to use bias in the linear layer\n",
    "        dropout = probability of dropout\n",
    "        \"\"\"\n",
    "\n",
    "        self.c_attn = nn.Linear(embedding_dim, 3 * embedding_dim, bias=bias)\n",
    "\n",
    "        self.output_projection = nn.Linear(embedding_dim, embedding_dim, bias=bias)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)).bool().unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, max_seq_len, _ = x.size() \n",
    "\n",
    "        #Â compute query, key and value vectors for all heads in a batch\n",
    "        #Â split the embedding dimension into query, key and value\n",
    "        Q, K, V = self.c_attn(x).split(self.embedding_dim, dim=2) # [batch_size, max_seq_len, embedding_dim]\n",
    "        \n",
    "        #Â reshape the query, key and value vectors to have a separate head for each token\n",
    "        Q = Q.view(batch_size, max_seq_len, self.num_heads, self.head_size).transpose(1, 2) # [batch_size, max_seq_len, num_heads, head_size]\n",
    "        K = K.view(batch_size, max_seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        V = V.view(batch_size, max_seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        attention = (Q @ K.transpose(-2, -1)) * (1.0/math.sqrt(K.size(-1))) # transpose swaps the last two dimensions of K = (1,5,24) @ (1,24,5) = (1,5,5)\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len)).bool().unsqueeze(0).unsqueeze(0)\n",
    "        attention = attention.masked_fill(~mask[:, :, :max_seq_len, :max_seq_len], float(\"-inf\"))  \n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        attention = self.attention_dropout(attention)\n",
    "\n",
    "        hidden_state = attention @ V # [batch_size, num_heads, max_seq_len, head_size]\n",
    "\n",
    "        hidden_state = hidden_state.transpose(1, 2).contiguous().view(batch_size, max_seq_len, self.embedding_dim)\n",
    "        hidden_state = self.resid_dropout(hidden_state)\n",
    "\n",
    "        return hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape before reshape: torch.Size([1, 19, 512])\n",
      "torch.Size([1, 19, 512])\n"
     ]
    }
   ],
   "source": [
    "# testing attention mask\n",
    "x = input_embeddings\n",
    "\n",
    "masked_attention = MaskedAttention(embedding_dim=512, head_size=64, max_seq_len=19, num_heads=8)\n",
    "\n",
    "output = masked_attention(x)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, bias=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, 4 * embedding_dim, bias=bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * embedding_dim, embedding_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, head_size, max_seq_len, num_heads=1, bias=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.masked_attention = MaskedAttention(embedding_dim, head_size, max_seq_len, num_heads, bias, dropout)\n",
    "        self.fnn = FNN(embedding_dim, bias, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.masked_attention(self.norm1(x))\n",
    "        x = x + self.fnn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 1, 20])\n",
      "Loss: None\n"
     ]
    }
   ],
   "source": [
    "# Decoder class \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", embedding_dim=512, num_heads=8, max_seq_len=19, size_of_vocab=20, num_layers=6, bias=False, dropout=0.2, head_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_model = CLIPTextModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.embedding_dim = self.clip_model.config.hidden_size\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.size_of_vocab = size_of_vocab\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.head_size = head_size\n",
    "\n",
    "\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            dropout = nn.Dropout(dropout),\n",
    "            blocks = nn.ModuleList([DecoderBlock(embedding_dim, head_size, max_seq_len, num_heads, bias, dropout) for _ in range(num_layers)]),\n",
    "            layer_norm = nn.LayerNorm(embedding_dim),\n",
    "            head = nn.Linear(embedding_dim, size_of_vocab, bias=bias)\n",
    "        ))\n",
    "\n",
    "    def forward(self, captions, targets=None):\n",
    "        caption_inputs = self.tokenizer(captions, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        input_ids = caption_inputs['input_ids']\n",
    "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        token_embeddings = self.clip_model.text_model.embeddings.token_embedding(input_ids)\n",
    "        position_embeddings = self.clip_model.text_model.embeddings.position_embedding(position_ids)\n",
    "\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.transformer['dropout'](x)\n",
    "\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "        x = self.transformer.layer_norm(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # compute the loss if we are given targets\n",
    "            logits = self.transformer['head'](x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # only look at last token if performing inference\n",
    "            logits = self.transformer.head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "decoder = TransformerDecoder()\n",
    "\n",
    "caption = captions[0][0]\n",
    "logits, loss = decoder(caption)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
