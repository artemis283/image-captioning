{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from decoder import TransformerDecoder\n",
    "import transformers\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# === Set up environment ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load CLIP & Tokenizer ===\n",
    "CLIP = transformers.CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "tokenizer = transformers.CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "CLIP.eval()\n",
    "for param in CLIP.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "token_embedding = CLIP.text_model.embeddings.token_embedding.to(device)\n",
    "\n",
    "# === Load token embedding ===\n",
    "token_embedding.weight.requires_grad = False\n",
    "\n",
    "flickr = load_dataset(\"flickr30k_dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# === Instantiate model ===\n",
    "model = TransformerDecoder().to(device)\n",
    "\n",
    "# === Load weights ===\n",
    "checkpoint = torch.load(\"decoder_9.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "def get_test_image():\n",
    "    test_image = flickr['test'][5]['image']\n",
    "    # test_cap = flickr['test'][0]['caption'][0]\n",
    "    plt.imshow(test_image)\n",
    "\n",
    "    # test_cap = tokenizer(test_cap,return_tensors=\"pt\", padding=\"max_length\", truncation=True)['input_ids']\n",
    "\n",
    "    # test_cap = token_embedding(test_cap)\n",
    "\n",
    "    # print (test_cap.shape)\n",
    "\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.4815, 0.4578, 0.4082],\n",
    "            std=[0.2686, 0.2613, 0.2758]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "    CLIP.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = CLIP.vision_model(preprocess(test_image).unsqueeze(0).to(device))\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    # print (patch_embeddings.shape)\n",
    "\n",
    "    img = patch_embeddings\n",
    "\n",
    "    return (img)\n",
    "\n",
    "# === Inference Function ===\n",
    "\n",
    "def inference(model, start_token=49406, end_token=49407, max_len=77, device='cuda'):\n",
    "    model.eval()\n",
    "     # Shape: (1, 4, 196)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Start with <start> token\n",
    "        generated = [start_token]\n",
    "        img = get_test_image()\n",
    "        for _ in range(max_len):\n",
    "            y = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "            y_emb = token_embedding(y).to(device)\n",
    "            # print (y_emb.shape)\n",
    "            # print (img.shape)\n",
    "            \n",
    "            logits = model(img, y_emb)# (1, seq_len, vocab_size)\n",
    "            next_token_logits = logits[0, -1]  # (vocab_size,)\n",
    "            next_token = torch.argmax(next_token_logits).item()\n",
    "            #print (next_token)\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            if next_token == end_token:\n",
    "                break\n",
    "\n",
    "    return generated[1:]\n",
    "\n",
    "\n",
    "# === Run Inference ===\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"example.jpg\"  # replace with your test image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    caption = inference(model)\n",
    "\n",
    "    print(\"\\nðŸ“· Generated caption:\", tokenizer.decode(caption))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
